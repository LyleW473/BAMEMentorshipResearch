import openai
import json
from os.path import exists as os_path_exists
from os import getenv
from os import mkdir
import itertools

def main():
    # Initialise API
    API_TYPE = getenv("API_TYPE")
    ENDPOINT = getenv("ENDPOINT")
    API_VERSION = getenv("API_VERSION")
    API_KEY = getenv("API_KEY")

    openai.api_type = API_TYPE
    openai.azure_endpoint = ENDPOINT
    openai.api_version = API_VERSION
    openai.api_key = API_KEY

    # Read question
    with open("prob1.txt") as p_file:
        contents = p_file.readlines()
        # print(contents)

    # Construct permutations of idxs (relating to the subquestions in the main question)
    idxs = [1, 2, 3, 4, 5, 6]
    all_permutations = [list(itertools.permutations(idxs, length)) for length in range(1, len(idxs) + 1)] # Contains sublists of permutations at a specific length
    flattened_permutations = [comb for sublist in all_permutations for comb in sublist] # Flattened to 1 list
    print(flattened_permutations)
    print(f"Num permutations: {len(flattened_permutations)}")

    queries = [contents[0] + ("".join(contents[idx] for idx in flattened_permutations[i])) for i in range(len(flattened_permutations))]

    for query in queries:
        print(query)
        print("----------------")

    # Generate answers to queries

    # Create folder for holding the json for results
    if not os_path_exists("results"):
        mkdir("results")

    mappings = {} # Maps the subquestions used (List of indexes) to the predictions generated by the LLM.
    for i, (query, idxs_used) in enumerate(zip(queries, flattened_permutations)):
        print(f"i: {i}\nQuery:\n{query}\nIndexes used:\n{idxs_used}\n")
        # print(i)
        message_text = [{
                        "role": "system",
                        "content": "You are an AI assistant that helps people find information."
                        },
                        {
                        "role": "user",
                        "content": query
                        }]
        
        answer = (openai.chat.completions.create(
                                                model = "gpt-4",
                                                messages = message_text,
                                                temperature = 0.7,
                                                max_tokens = 800,
                                                top_p = 0.95,
                                                frequency_penalty = 0,
                                                presence_penalty = 0,
                                                stop = None,
                                                )).choices[0].message.content # Extract Python String
        
        # Create hashmap mapping
        indexes_string = "".join(str(idx) + "#" for idx in idxs_used) # Used because cannot have keys as tuple or list when using json.dump
        mappings[indexes_string] = answer
        
        # Save results
        with open("results/prob1_predictions.json", "w") as file:
            json.dump(mappings, file)

if __name__ == "__main__":
    main()